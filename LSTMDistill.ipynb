{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import uuid\n",
    "import faiss\n",
    "import json\n",
    "from utils.EEGDataset import EEGDataset\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, channels=128, n_layers=2, out_features=384):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_init = (torch.zeros(self.n_layer, batch_size, self.hidden_size), torch.zeros(self.n_layer, batch_size, self.hidden_size))\n",
    "        if x.is_cuda: lstm_init = (lstm_init[0].cuda(), lstm_init[0].cuda())\n",
    "        lstm_init = (Variable(lstm_init[0], volatile=x.volatile), Variable(lstm_init[1], volatile=x.volatile))\n",
    "\n",
    "        # Forward LSTM and get final state\n",
    "        x = self.lstm(x, lstm_init)[0][:,-1,:]\n",
    "        # x = F.softmax(self.fc(x))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "        # h0 = torch.zeros(self.n_layer, x.size(0), self.hidden_size)\n",
    "        # c0 = torch.zeros(self.n_layer, x.size(0), self.hidden_size)\n",
    "        # lstm_out, hidden_out = self.lstm(x, (h0, c0))\n",
    "        # # out = self.fc(lstm_out[:, -1, :])\n",
    "        # return lstm_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{39: 'Egyptian_cat', 35: 'African_elephant', 0: 'sorrel', 21: 'capuchin', 8: 'giant_panda', 12: 'German_shepherd', 7: 'revolver', 30: 'grand_piano', 36: 'airliner', 10: 'canoe', 20: 'missile', 6: 'mountain_bike', 37: 'electric_locomotive', 24: 'convertible', 25: 'folding_chair', 22: 'pool_table', 32: 'banana', 28: 'electric_guitar', 9: 'daisy', 3: 'anemone_fish', 34: 'digital_watch', 38: 'radio_telescope', 17: 'desktop_computer', 14: \"jack-o'-lantern\", 11: 'lycaenid', 2: 'iron', 4: 'espresso_maker', 31: 'mountain_tent', 26: 'pajama', 13: 'running_shoe', 16: 'golf_ball', 23: 'mailbag', 18: 'broom', 27: 'mitten', 15: 'cellular_telephone', 1: 'parachute', 19: 'pizza', 29: 'reflex_camera', 33: 'bolete', 5: 'coffee_mug'}\n",
      "Transforming data to channel wise norm across labels\n",
      "Transforming data to channel wise norm across labels (done)\n",
      "{39: 'Egyptian_cat', 35: 'African_elephant', 0: 'sorrel', 21: 'capuchin', 8: 'giant_panda', 12: 'German_shepherd', 7: 'revolver', 30: 'grand_piano', 36: 'airliner', 10: 'canoe', 20: 'missile', 6: 'mountain_bike', 37: 'electric_locomotive', 24: 'convertible', 25: 'folding_chair', 22: 'pool_table', 32: 'banana', 28: 'electric_guitar', 9: 'daisy', 3: 'anemone_fish', 34: 'digital_watch', 38: 'radio_telescope', 17: 'desktop_computer', 14: \"jack-o'-lantern\", 11: 'lycaenid', 2: 'iron', 4: 'espresso_maker', 31: 'mountain_tent', 26: 'pajama', 13: 'running_shoe', 16: 'golf_ball', 23: 'mailbag', 18: 'broom', 27: 'mitten', 15: 'cellular_telephone', 1: 'parachute', 19: 'pizza', 29: 'reflex_camera', 33: 'bolete', 5: 'coffee_mug'}\n",
      "Transforming data to channel wise norm across labels\n",
      "Transforming data to channel wise norm across labels (done)\n"
     ]
    }
   ],
   "source": [
    "SUBJECT = 1\n",
    "BATCH_SIZE = 8\n",
    "learning_rate = 0.0001\n",
    "EPOCHS = 50\n",
    "SaveModelOnEveryEPOCH = 100\n",
    "EEG_DATASET_PATH = \"./data/eeg/eeg_signals_raw_with_mean_std.pth\"\n",
    "EEG_DATASET_SPLIT = \"./data/eeg/block_splits_by_image_all.pth\"\n",
    "\n",
    "LSTM_INPUT_FEATURES = 128 # should be image features output.\n",
    "LSTM_HIDDEN_SIZE = 460  # should be same as sequence length\n",
    "selectedDataset = \"imagenet40\"\n",
    "\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(256, antialias=True),       \n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "dataset = EEGDataset(subset=\"train\",\n",
    "                         eeg_signals_path=EEG_DATASET_PATH,\n",
    "                         eeg_splits_path=EEG_DATASET_SPLIT, \n",
    "                         subject=SUBJECT,\n",
    "                         time_low=20,\n",
    "                         time_high=480,\n",
    "                         exclude_subjects=[],\n",
    "                         convert_image_to_tensor=False,\n",
    "                         apply_channel_wise_norm=True,\n",
    "                         preprocessin_fn=transform_image)\n",
    "\n",
    "\n",
    "val_dataset = EEGDataset(subset=\"val\",\n",
    "                         eeg_signals_path=EEG_DATASET_PATH,\n",
    "                         eeg_splits_path=EEG_DATASET_SPLIT, \n",
    "                         subject=SUBJECT,\n",
    "                         time_low=20,\n",
    "                         time_high=480,\n",
    "                         exclude_subjects=[],\n",
    "                         convert_image_to_tensor=False,\n",
    "                         apply_channel_wise_norm=True,\n",
    "                         preprocessin_fn=transform_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ASUS/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\ASUS/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\ASUS/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\ASUS/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "def initDinoV2Model(model= \"dinov2_vits14\"):\n",
    "    dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", model)\n",
    "    return dinov2_vits14\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dinov2_model = initDinoV2Model(model=\"dinov2_vits14\").to(device)\n",
    "dinov2_model = dinov2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run the code on one GPU.\n",
      "| distributed init (rank 0): env://\n"
     ]
    }
   ],
   "source": [
    "from utils import utils\n",
    "\n",
    "class FLAGS:\n",
    "    num_workers = 4\n",
    "    dist_url = \"env://\"\n",
    "    local_rank = 0\n",
    "\n",
    "utils.init_distributed_mode(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "dataset.extract_features(model=dinov2_model, data_loader=data_loader_train, replace_eeg=False)\n",
    "val_dataset.extract_features(model=dinov2_model, data_loader=data_loader_val, replace_eeg=False)\n",
    "\n",
    "# data_loader_train = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=4,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=False,\n",
    "# )\n",
    "\n",
    "# data_loader_val = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=4,\n",
    "#     pin_memory=True,\n",
    "#     drop_last=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg, label,image,i, image_features = next(iter(data_loader_train)) \n",
    "outs = dinov2_model(image.to(device))\n",
    "features_length = outs.size(-1)\n",
    "print(outs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=LSTM_INPUT_FEATURES,hidden_size=LSTM_HIDDEN_SIZE,channels=128, out_features=features_length)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(eeg.to(device))\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_lstm, y_resnet):\n",
    "        # Calculate the Euclidean distance between y_lstm and y_resnet\n",
    "        loss = torch.mean(torch.square(y_lstm - y_resnet))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paramters:\n",
    "    alpha = 1\n",
    "    temperature = 0.977\n",
    "\n",
    "\n",
    "def loss_fn_kd(outputs, labels, teacher_outputs, params):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    alpha = params.alpha\n",
    "    T = params.temperature\n",
    "    # KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "    #                          F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
    "    #           F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) \n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = CustomLoss()\n",
    "opt = torch.optim.Adam(lr=0.0001, params=model.parameters())\n",
    "# criterion = \n",
    "\n",
    "epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "for EPOCH in range(40):\n",
    "\n",
    "    batch_losses = []\n",
    "    val_batch_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data in data_loader_train:\n",
    "        eeg, label,image,i, image_features = data\n",
    "\n",
    "        image_features = torch.from_numpy(np.array(image_features)).to(device)\n",
    "        # print(image_features.size())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        lstm_output = model(eeg.to(device))\n",
    "        # dinov2_out = dinov2_model(image.to(device))\n",
    "\n",
    "        # loss = criterion(image_features, lstm_output)\n",
    "        loss = loss_fn_kd(outputs=lstm_output,labels=None,teacher_outputs=image_features, params=Paramters)\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for data in data_loader_val:\n",
    "        eeg, label,image,i, image_features = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = torch.from_numpy(np.array(image_features)).to(device)\n",
    "            lstm_output = model(eeg.to(device))\n",
    "            # loss = criterion(image_features, lstm_output)\n",
    "            loss = loss_fn_kd(outputs=lstm_output,labels=None,teacher_outputs=image_features, params=Paramters)\n",
    "            val_batch_losses.append(loss.item())\n",
    "    \n",
    "    batch_losses = np.array(batch_losses)\n",
    "    val_batch_losses = np.array(val_batch_losses)\n",
    "    val_epoch_loss= val_batch_losses.mean()\n",
    "    epoch_loss = batch_losses.mean()\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    val_epoch_losses.append(val_epoch_loss)\n",
    "\n",
    "    print(f\"EPOCH {EPOCH} train_loss: {epoch_loss} val_loss: {val_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"output/lstm_dinov2_distilled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_features = []\n",
    "lstm_features_labels = []\n",
    "\n",
    "\n",
    "test_dataset = EEGDataset(subset=\"train\",\n",
    "                         eeg_signals_path=EEG_DATASET_PATH,\n",
    "                         eeg_splits_path=EEG_DATASET_SPLIT, \n",
    "                         subject=SUBJECT,\n",
    "                         time_low=20,\n",
    "                         time_high=480,\n",
    "                         exclude_subjects=[],\n",
    "                         convert_image_to_tensor=False,\n",
    "                         preprocessin_fn=transform_image)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for data in test_dataset:\n",
    "    eeg, label,image,i, image_features = data\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # image_features = torch.from_numpy(np.array(image_features)).to(device)\n",
    "        lstm_output = model(eeg.unsqueeze(0).to(device))\n",
    "        # loss = criterion(image_features, lstm_output)\n",
    "        lstm_features.append(lstm_output.cpu().numpy())\n",
    "        lstm_features_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_features_labels_int = []\n",
    "for label in lstm_features_labels:\n",
    "    lstm_features_labels_int.append(label[\"ClassId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_features = np.array(lstm_features)\n",
    "lstm_features = lstm_features.reshape(len(lstm_features_labels_int),-1)\n",
    "lstm_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_sample0_time_RAW = TSNE(n_components=3,perplexity=40, init=\"pca\", learning_rate=0.1, n_iter=300).fit_transform(lstm_features)\n",
    "\n",
    "handles = []\n",
    "cmaps = []\n",
    "gen_colors = []\n",
    "cmap = plt.cm.get_cmap(\"hsv\",len(list(set(lstm_features_labels_int))))\n",
    "for eeg_label in list(set(lstm_features_labels_int)):\n",
    "    _patch = mpatches.Patch(color=cmap(eeg_label), label=f'Class {eeg_label}') \n",
    "    cmaps.append(cmap(eeg_label))\n",
    "    handles.append(_patch)\n",
    "for i in range(lstm_features.shape[0]):\n",
    "    colorMap = cmaps[lstm_features_labels_int[i]]\n",
    "    gen_colors.append(colorMap)\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.set_size_inches(20,20)\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "\n",
    "ax.set_title(\"EEG data\")\n",
    "# ax.view_init(azim=90, elev=1)\n",
    "ax.view_init(azim=60, elev=30)\n",
    "_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n",
    "\n",
    "# sel_channel = 97\n",
    "ax.scatter(X_tsne_sample0_time_RAW[:,0], X_tsne_sample0_time_RAW[:,1], X_tsne_sample0_time_RAW[:,2], c=gen_colors, s=30, alpha=0.8)\n",
    "ax.legend(handles=handles, loc=\"best\", fontsize=13, bbox_to_anchor=(1.2, 0.1),fancybox=True,ncol=5)\n",
    "# fig.savefig(f\"./output/AprilTsneAnalysis/Channel_{SelectedChannels[0]}_start{TimeStart}_end{TimeEnd}.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
